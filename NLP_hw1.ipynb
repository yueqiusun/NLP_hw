{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge all training and testing file into one file respectively\n",
    "#!for split in train test; do for sentiment in pos neg; do for file in aclImdb/$split/$sentiment/*; do cat $file >> aclImdb/movie_data/full_${split}_$sentiment.txt; echo >> aclImdb/movie_data/full_${split}_$sentiment.txt; done; done; done;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 ./aclImdb/movie_data/full_train_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build train_data_tokens.p, all_train_tokens.p, val_data_tokens.p, test_data_tokens.p\n",
    "#!python load_data.py   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from module import *\n",
    "import prep\n",
    "import load_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "prepath_data = './aclImdb/movie_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    token = nltk.word_tokenize(sent)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepath_data = './aclImdb/movie_data/'\n",
    "tp_filename = 'full_train_pos.txt'\n",
    "tn_filename = 'full_train_neg.txt'   #train_neg_filename\n",
    "\n",
    "movie_train_data, movie_train_target = prep.read_data(prepath_data + tp_filename, prepath_data + tn_filename,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def word_grams(sent, min=1, max=4):\n",
    "    words = sent.split(' ')\n",
    "    s = []\n",
    "    for n in range(min, max):\n",
    "        for ngram in ngrams(words, n):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'an', 'Apple', 'I have', 'have an', 'an Apple', 'I have an', 'have an Apple']\n"
     ]
    }
   ],
   "source": [
    "sent = 'I have an Apple'\n",
    "print(word_grams(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4742387\n"
     ]
    }
   ],
   "source": [
    "# load preprocessed train, val and test datasets\n",
    "train_data_tokens = pkl.load(open(prepath_data + \"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(prepath_data + \"all_train_tokens.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(prepath_data + \"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(prepath_data + \"test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 1346 ; token slasher\n",
      "Token slasher; token id 1346\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create PyTorch DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from newsgroupdataset import NewsGroupDataset\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define Bag-of-Words model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "from bagofwords import BagOfWords\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 56.7\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 67.24\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 79.5\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 67.24\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 76.86\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 86.94\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 81.74\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 76.28\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 81.62\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 80.94\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 81.08\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 78.82\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 79.74\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 78.12\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 86.3\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 73.88\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 86.08\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 74.98\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 80.78\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 72.7\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 79.62\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 75.4\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 82.22\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 78.18\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 80.28\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 78.08\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 71.8\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 75.64\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 78.1\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 84.26\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 78.72\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 78.32\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 73.56\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 81.8\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 80.56\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 77.86\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 80.98\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 78.3\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 77.46\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 69.92\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 76.86\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 75.8\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 79.36\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 79.86\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 71.9\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 77.92\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 78.28\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 81.1\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 71.46\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 74.76\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 78.52\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 77.62\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 79.2\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 76.82\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 75.54\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 74.96\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 77.0\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 78.1\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 78.0\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 78.4\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 77.44\n",
      "Test Acc 80.328\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 1\n",
    "### Try training the model with larger embedding size and for larger number of epochs\n",
    "### Also plot the training curves of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "### Try downloading IMDB Large Movie Review Dataset that is used for Assignment 1 http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "### and tokenize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "### If you have time, after tokenizing the dataset try training Bag-of-Words model on it and report your initial results\n",
    "### on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
