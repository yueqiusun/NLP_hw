{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge all training and testing file into one file respectively\n",
    "#!for split in train test; do for sentiment in pos neg; do for file in aclImdb/$split/$sentiment/*; do cat $file >> aclImdb/movie_data/full_${split}_$sentiment.txt; echo >> aclImdb/movie_data/full_${split}_$sentiment.txt; done; done; done;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 ./aclImdb/movie_data/full_train_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build train_data_tokens.p, all_train_tokens.p, val_data_tokens.p, test_data_tokens.p\n",
    "#!python load_data.py   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from module import *\n",
    "import prep\n",
    "import load_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "prepath_data = './aclImdb/movie_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from module import *\n",
    "import prep\n",
    "\n",
    "prepath_data = './aclImdb/movie_data/'\n",
    "tp_filename = 'full_train_pos.txt'   #train_pos_filename\n",
    "tn_filename = 'full_train_neg.txt'   #train_neg_filename\n",
    "tep_filename = 'full_test_pos.txt'    #test_pos_filename\n",
    "ten_filename = 'full_test_neg.txt'    #test_neg_filename\n",
    "\n",
    "\n",
    "\n",
    "movie_train_data, movie_train_target = prep.read_data(prepath_data + tp_filename, prepath_data + tn_filename)\n",
    "movie_test_data, movie_test_target = prep.read_data(prepath_data + tep_filename, prepath_data + ten_filename)\n",
    "\n",
    "train_split = 20000\n",
    "train_data = movie_train_data[:train_split]\n",
    "train_targets = movie_train_target[:train_split]\n",
    "\n",
    "val_data = movie_train_data[train_split:]\n",
    "val_targets = movie_train_target[train_split:]\n",
    "\n",
    "test_data = movie_test_data\n",
    "test_targets = movie_test_target\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#n-gram features for texts\n",
    "def f_ngram(train, test, mode='tfidf', binary=1, ngram=(1,1), min_c=1):\n",
    "    '''exact n-gram feacturs\n",
    "    return: feature array (fgram); feature vocabulary (vocab)\n",
    "    input: raw train and test data; ngram = (n,n) denote n_gram and ngram=(1,2) denote\n",
    "    1_gram and 2_gram; tokens with count below min_c are cut off. \n",
    "    '''\n",
    "    if mode == 'tfidf':\n",
    "        if binary==1:\n",
    "            gram = text.TfidfVectorizer(ngram_range=ngram, binary=True, min_df=min_c)\n",
    "        else:\n",
    "            gram = text.TfidfVectorizer(ngram_range=ngram, min_df=min_c)\n",
    "        \n",
    "    else: #mode=count\n",
    "        if binary==1:\n",
    "            gram = text.CountVectorizer(ngram_range=ngram, binary=True, min_df=min_c)\n",
    "        else:\n",
    "            gram = text.CountVectorizer(ngram_range=ngram, min_df=min_c)\n",
    "    train = dataFilter_train(train)\n",
    "    gram = gram.fit(train)\n",
    "    vocab = gram.get_feature_names()\n",
    "    fgram_train = gram.transform(train).toarray()\n",
    "    test = dataFilter(test, vocab)\n",
    "    fgram_test = gram.transform(test).toarray()\n",
    "    return (fgram_train, fgram_test, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing test data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = prep.tokenize_dataset(val_data)\n",
    "pkl.dump(val_data_tokens, open(prepath_data + \"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = prep.tokenize_dataset(test_data)\n",
    "pkl.dump(test_data_tokens, open(prepath_data + \"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "#train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = prep.tokenize_dataset(train_data)\n",
    "pkl.dump(train_data_tokens, open(prepath_data + \"train_data_tokens.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(prepath_data + \"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4742387\n"
     ]
    }
   ],
   "source": [
    "# load preprocessed train, val and test datasets\n",
    "train_data_tokens = pkl.load(open(prepath_data + \"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(prepath_data + \"all_train_tokens.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(prepath_data + \"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(prepath_data + \"test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 7820 ; token life as\n",
      "Token life as; token id 7820\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create PyTorch DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from newsgroupdataset import NewsGroupDataset\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define Bag-of-Words model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "from bagofwords import BagOfWords\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 60.58\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 76.62\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 75.88\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 77.38\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 68.32\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 70.2\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 80.26\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 90.12\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 76.1\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 74.94\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 80.92\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 83.9\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 71.02\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 68.08\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 77.64\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 72.7\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 77.5\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 87.06\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 76.48\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 73.42\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 78.44\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 72.1\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 79.74\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 80.0\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 72.46\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 79.36\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 76.12\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 73.0\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 78.26\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 81.3\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 86.48\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 77.6\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 79.56\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 70.46\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 73.8\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 74.98\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 77.34\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 75.38\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 79.42\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 74.86\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 79.98\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 74.3\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 77.1\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 73.3\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 71.44\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 78.08\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 83.52\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 76.1\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 74.54\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 78.24\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 75.66\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 81.4\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 77.78\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 75.06\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 75.92\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 74.88\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 76.28\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 72.26\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 79.08\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 76.22\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 77.44\n",
      "Test Acc 80.328\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 1\n",
    "### Try training the model with larger embedding size and for larger number of epochs\n",
    "### Also plot the training curves of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "### Try downloading IMDB Large Movie Review Dataset that is used for Assignment 1 http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "### and tokenize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "### If you have time, after tokenizing the dataset try training Bag-of-Words model on it and report your initial results\n",
    "### on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
